{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accf1371",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing body texts (training & evaluation) <br>\n",
    "\n",
    "In the following, we preprocess the translated body text of both the training and the evaluation datasets and we export the results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40d01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "this_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d8e88",
   "metadata": {},
   "source": [
    "Let's import the translated (train) files from a .csv files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372cb4a4",
   "metadata": {},
   "source": [
    "The chosen preprocessing routine is the following:\n",
    "\n",
    "1. Lowercasing\n",
    "2. Punctuation removal\n",
    "3. Tokenization\n",
    "4. Stopword removal\n",
    "5. Lemmatization\n",
    "\n",
    "The first 4 steps of the process use the preprocessing tools of [NLTK].\n",
    "(https://www.nltk.org/).  \n",
    "For lemmatization, the ad-hoc tool from [spaCy](https://spacy.io/) was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dfc77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_body(text):\n",
    "    \n",
    "    #lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    #punctuation removal\n",
    "    text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    #tokenize\n",
    "    words = word_tokenize(text_p)\n",
    "    \n",
    "    #stopword removal\n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    #lemmatization\n",
    "    filtered_temp = ' '.join(filtered_words)\n",
    "    doc = nlp(filtered_temp)\n",
    "    lemmatized_output = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b049e67",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocess body text (evaluation) <br>\n",
    "\n",
    "We preprocess the translated body text of the articles in the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e352ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4902\n"
     ]
    }
   ],
   "source": [
    "eval_data = pd.read_csv(this_dir + '\\eval\\_EVAL_text_translated.csv')\n",
    "\n",
    "nan_list1 =  eval_data[(eval_data['translated_body1'].isna())].index.tolist()\n",
    "\n",
    "nan_list2 = eval_data[(eval_data['translated_body2'].isna())].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a2065c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_eval1 = [preprocess_body(str(i)) for i in eval_data[\"translated_body1\"].tolist()]\n",
    "preprocessed_eval2 = [preprocess_body(str(i)) for i in eval_data[\"translated_body2\"].tolist()]\n",
    "\n",
    "eval_data[\"preprocessed_1\"] = preprocessed_eval1\n",
    "eval_data[\"preprocessed_2\"] = preprocessed_eval2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fab1f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/eval/_EVAL_preprocessed_text.csv'\n",
    "eval_data.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0606e",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocess body text (training) <br>\n",
    "\n",
    "We preprocess the translated body text of the articles in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4440b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(this_dir + '\\train\\_TRAIN_text_translated')\n",
    "#train_data = pd.read_csv(this_dir + '\\eval_text_translations\\train_fully_translated.csv')\n",
    "\n",
    "nan_list1 =  train_data[(train_data['translated_body1'].isna())].index.tolist()\n",
    "\n",
    "nan_list2 = train_data[(train_data['translated_body2'].isna())].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0eb0d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train1 = [preprocess_body(str(i)) for i in train_data[\"translated_body1\"].tolist()]\n",
    "preprocessed_train2 = [preprocess_body(str(i)) for i in train_data[\"translated_body2\"].tolist()]\n",
    "\n",
    "train_data[\"preprocessed_1\"] = preprocessed_train1\n",
    "train_data[\"preprocessed_2\"] = preprocessed_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5384b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/train/_TRAIN_preprocessed_text'\n",
    "train_data.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32901852",
   "metadata": {},
   "source": [
    "An example of use of the spaCy lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733a292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will go and I be and I go and I be there and here and nowhere .\n"
     ]
    }
   ],
   "source": [
    "text = \"I will go and I am and I went and I was there and here and nowhere.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "lemmatized_out = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "print(lemmatized_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bf557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
